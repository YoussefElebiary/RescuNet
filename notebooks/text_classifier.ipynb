{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2376171,"sourceType":"datasetVersion","datasetId":1435855},{"sourceId":13925220,"sourceType":"datasetVersion","datasetId":8873708},{"sourceId":13928658,"sourceType":"datasetVersion","datasetId":8875982}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport os\nimport re\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom collections import Counter\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-06T06:31:27.549757Z","iopub.execute_input":"2025-12-06T06:31:27.549982Z","iopub.status.idle":"2025-12-06T06:31:33.263942Z","shell.execute_reply.started":"2025-12-06T06:31:27.549941Z","shell.execute_reply":"2025-12-06T06:31:33.263356Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Importing datasets and doing some labeling","metadata":{}},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\")\ndf_original = pd.read_csv(\"/kaggle/input/training-set-fakenews/train.csv\")[['text', 'target']].rename(columns={'target': 'label'})\nprint(f\"Kaggle dataset: {len(df_original)} rows\")\ncrisisnlp_paths = [\n    \"/kaggle/input/dataset-for-crisisnlp/2013_Pakistan_eq_CF_labeled_data.tsv\",\n    \"/kaggle/input/dataset-for-crisisnlp/2014_California_Earthquake_CF_labeled_data.tsv\",\n    \"/kaggle/input/dataset-for-crisisnlp/2014_Hurricane_Odile_Mexico_en_CF_labeled_data.tsv\",\n    \"/kaggle/input/dataset-for-crisisnlp/2014_Chile_Earthquake_en_CF_labeled_data.tsv\",\n    \"/kaggle/input/dataset-for-crisisnlp/2014_India_floods_CF_labeled_data.tsv\",\n    \"/kaggle/input/dataset-for-crisisnlp/2014_MERS_en_CF_labeled_data.tsv\",\n    \"/kaggle/input/dataset-for-crisisnlp/2014_Pakistan_floods_CF_labeled_data.tsv\",\n    \"/kaggle/input/dataset-for-crisisnlp/2014_Philippines_Typhoon_Hagupit_en_CF_labeled_data.tsv\",\n    \"/kaggle/input/dataset-for-crisisnlp/2014_ebola_CF_labeled_data.tsv\",\n    \"/kaggle/input/dataset-for-crisisnlp/2015_Cyclone_Pam_en_CF_labeled_data.tsv\",\n    \"/kaggle/input/dataset-for-crisisnlp/2015_Nepal_Earthquake_en_CF_labeled_data.tsv\",\n]\nirrelevant_labels = ['not_related_or_irrelevant', 'sympathy_and_emotional_support']\nall_crisisnlp = []\nfor filepath in crisisnlp_paths:\n    df_temp = pd.read_csv(filepath, sep='\\t', usecols=[1, 2],names=['text', 'original_label'], header=0)\n    df_temp['label'] = df_temp['original_label'].apply(lambda x: 0 if x in irrelevant_labels else 1)\n    all_crisisnlp.append(df_temp[['text', 'label']])\ndf_crisisnlp = pd.concat(all_crisisnlp, ignore_index=True)\nprint(f\"CrisisNLP combination data: {len(df_crisisnlp)} rows\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T06:31:33.265581Z","iopub.execute_input":"2025-12-06T06:31:33.266233Z","iopub.status.idle":"2025-12-06T06:31:33.550624Z","shell.execute_reply.started":"2025-12-06T06:31:33.266213Z","shell.execute_reply":"2025-12-06T06:31:33.550009Z"}},"outputs":[{"name":"stdout","text":"Kaggle dataset: 8272 rows\nCrisisNLP combination data: 20514 rows\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Concatenating both datasets and randomizing it","metadata":{}},{"cell_type":"code","source":"df_all = pd.concat([df_original, df_crisisnlp], ignore_index=True).drop_duplicates(subset=['text'])\ndf_all = df_all.sample(frac=1, random_state=42).reset_index(drop=True)\nprint(f\"\\nTotal clean data: {len(df_all)} rows\")\nprint(\"Label distribution:\")\nprint(df_all['label'].value_counts())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T06:31:33.553319Z","iopub.execute_input":"2025-12-06T06:31:33.553671Z","iopub.status.idle":"2025-12-06T06:31:33.572286Z","shell.execute_reply.started":"2025-12-06T06:31:33.553645Z","shell.execute_reply":"2025-12-06T06:31:33.571532Z"}},"outputs":[{"name":"stdout","text":"\nTotal clean data: 28662 rows\nLabel distribution:\nlabel\n1    19353\n0     9309\nName: count, dtype: int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Calculating and Applying Class Weights","metadata":{}},{"cell_type":"code","source":"counts = df_all['label'].value_counts().sort_index()\ntotal = len(df_all)\nweight_0 = total / (2 * counts.get(0, 1))\nweight_1 = total / (2 * counts.get(1, 1))\nclass_weights = torch.tensor([weight_0, weight_1], dtype=torch.float32).to(DEVICE) \nprint(f\"Class weights: [{weight_0:.3f}, {weight_1:.3f}]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T06:31:57.506624Z","iopub.execute_input":"2025-12-06T06:31:57.507422Z","iopub.status.idle":"2025-12-06T06:31:57.755796Z","shell.execute_reply.started":"2025-12-06T06:31:57.507389Z","shell.execute_reply":"2025-12-06T06:31:57.755025Z"}},"outputs":[{"name":"stdout","text":"Class weights: [1.539, 0.741]\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Train and test splitting","metadata":{}},{"cell_type":"code","source":"train_df, val_df = train_test_split(df_all, test_size=0.20, random_state=42, stratify=df_all['label'])\nprint(f\"Training: {len(train_df)} samples | Validation: {len(val_df)} samples\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T06:31:59.759170Z","iopub.execute_input":"2025-12-06T06:31:59.759794Z","iopub.status.idle":"2025-12-06T06:31:59.786432Z","shell.execute_reply.started":"2025-12-06T06:31:59.759763Z","shell.execute_reply":"2025-12-06T06:31:59.785671Z"}},"outputs":[{"name":"stdout","text":"Training: 22929 samples | Validation: 5733 samples\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Normalization & Tokenization & Sequences","metadata":{}},{"cell_type":"code","source":"MAX_SEQ_LEN = 128\nVOCAB_SIZE = 20000\ndef clean_text(text):\n    text = str(text).lower()\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n    text = re.sub(r'[^\\w\\s]', '', text)\n    return text\ndef text_to_indices(text, vocab, max_len):\n    words = clean_text(text).split()\n    indices = [vocab.get(word, vocab['<UNK>']) for word in words]\n    if len(indices) > MAX_SEQ_LEN:\n        indices = indices[:MAX_SEQ_LEN]\n    return indices\ndef pad_sequences(sequences, max_len, pad_value=0):\n    padded = []\n    for seq in sequences:\n        if len(seq) < MAX_SEQ_LEN:\n            seq = seq + [pad_value] * (max_len - len(seq))\n        padded.append(seq)\n    return padded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T06:32:01.660783Z","iopub.execute_input":"2025-12-06T06:32:01.661295Z","iopub.status.idle":"2025-12-06T06:32:01.666895Z","shell.execute_reply.started":"2025-12-06T06:32:01.661273Z","shell.execute_reply":"2025-12-06T06:32:01.666145Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"word_counts = Counter(word for text in train_df['text'] for word in clean_text(text).split())\nvocab = {'<PAD>': 0, '<UNK>': 1}\nfor word, count in word_counts.most_common(VOCAB_SIZE - 2):\n    vocab[word] = len(vocab)\nprint(f\"Vocabulary size: {len(vocab)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T06:32:03.327201Z","iopub.execute_input":"2025-12-06T06:32:03.327500Z","iopub.status.idle":"2025-12-06T06:32:03.588829Z","shell.execute_reply.started":"2025-12-06T06:32:03.327477Z","shell.execute_reply":"2025-12-06T06:32:03.588024Z"}},"outputs":[{"name":"stdout","text":"Vocabulary size: 20000\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Making Tensors","metadata":{}},{"cell_type":"code","source":"train_sequences = train_df['text'].apply(lambda x: text_to_indices(x, vocab, MAX_SEQ_LEN)).tolist()\nval_sequences = val_df['text'].apply(lambda x: text_to_indices(x, vocab, MAX_SEQ_LEN)).tolist()\n\ntrain_padded = pad_sequences(train_sequences, MAX_SEQ_LEN, vocab['<PAD>'])\nval_padded = pad_sequences(val_sequences, MAX_SEQ_LEN, vocab['<PAD>'])\n\ntrain_data = torch.tensor(train_padded, dtype=torch.long)\ntrain_labels = torch.tensor(train_df['label'].values, dtype=torch.long)\nval_data = torch.tensor(val_padded, dtype=torch.long)\nval_labels = torch.tensor(val_df['label'].values, dtype=torch.long)\n\nprint(f\"Training tensor shape: {train_data.shape}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T06:32:05.102882Z","iopub.execute_input":"2025-12-06T06:32:05.103575Z","iopub.status.idle":"2025-12-06T06:32:05.850236Z","shell.execute_reply.started":"2025-12-06T06:32:05.103548Z","shell.execute_reply":"2025-12-06T06:32:05.848820Z"}},"outputs":[{"name":"stdout","text":"Training tensor shape: torch.Size([22929, 128])\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Bi-LSTM Classifier","metadata":{}},{"cell_type":"code","source":"EMBEDDING_DIM = 200\nHIDDEN_DIM = 100\nDROPOUT_RATE = 0.50\nNUM_LAYERS = 2\n\nclass SimpleClassifier(nn.Module):\n    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, dropout_rate, pad_idx):\n        super().__init__()  \n        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n        self.dropout = nn.Dropout(dropout_rate)\n        self.lstm = nn.LSTM(\n            embedding_dim, \n            hidden_dim, \n            num_layers=num_layers,\n            bidirectional=True, \n            batch_first=True, \n            dropout=dropout_rate if num_layers > 1 else 0\n        )\n        self.fc = nn.Linear(hidden_dim * 2, 2)\n        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n    def forward(self, x, use_temperature=False):\n        embedded = self.dropout(self.embedding(x))\n        lstm_out, _ = self.lstm(embedded)\n        lstm_out = lstm_out.permute(0, 2, 1)\n        pooled = F.max_pool1d(lstm_out, lstm_out.size(2)).squeeze(2)\n        logits = self.fc(self.dropout(pooled))\n        return logits\n        \nmodel = SimpleClassifier(\n    vocab_size=len(vocab),\n    embedding_dim=EMBEDDING_DIM,\n    hidden_dim=HIDDEN_DIM,\n    num_layers=NUM_LAYERS,\n    dropout_rate=DROPOUT_RATE,\n    pad_idx=vocab['<PAD>']\n).to(DEVICE)\n\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"\\nModel parameters: {total_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T06:32:07.604299Z","iopub.execute_input":"2025-12-06T06:32:07.604588Z","iopub.status.idle":"2025-12-06T06:32:07.755055Z","shell.execute_reply.started":"2025-12-06T06:32:07.604568Z","shell.execute_reply":"2025-12-06T06:32:07.754300Z"}},"outputs":[{"name":"stdout","text":"\nModel parameters: 4,483,603\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Model Training and Evaluation","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 32\nepochs = 100\nLEARNING_RATE = 1e-3\nWEIGHT_DECAY = 0.001\nPATIENCE = 5\n\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\nbest_val_loss = float('inf')\nbest_val_f1 = 0.0\npatience_counter = 0\n\nfor epoch in range(epochs):\n    start_time = time.time()\n\n    model.train()\n    running_train_loss = 0.0\n    y_train_preds = []\n    y_train_labels = []\n\n    perm = torch.randperm(len(train_data))\n    train_data_shuffled = train_data[perm]\n    train_labels_shuffled = train_labels[perm]\n\n    num_batches = 0\n    for i in range(0, len(train_data), BATCH_SIZE):\n        batch_data = train_data_shuffled[i:i+BATCH_SIZE].to(DEVICE)\n        batch_labels = train_labels_shuffled[i:i+BATCH_SIZE].to(DEVICE)\n\n        optimizer.zero_grad()\n        logits = model(batch_data)\n        loss = criterion(logits, batch_labels)\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n\n        running_train_loss += loss.item()\n        num_batches += 1\n\n        probs = torch.softmax(logits, dim=1)\n        predictions = torch.argmax(probs, dim=1)\n        \n        y_train_preds.extend(predictions.cpu().numpy())\n        y_train_labels.extend(batch_labels.cpu().numpy())\n\n    avg_train_loss = running_train_loss / num_batches\n    \n    train_f1 = f1_score(y_train_labels, y_train_preds, zero_division=0)\n    train_precision = precision_score(y_train_labels, y_train_preds, zero_division=0)\n    train_recall = recall_score(y_train_labels, y_train_preds, zero_division=0)\n\n    model.eval()\n    running_val_loss = 0.0\n    y_val_preds = []\n    y_val_labels = []\n\n    with torch.no_grad():\n        num_val_batches = 0\n        for i in range(0, len(val_data), BATCH_SIZE):\n            batch_data = val_data[i:i+BATCH_SIZE].to(DEVICE)\n            batch_labels = val_labels[i:i+BATCH_SIZE].to(DEVICE)\n\n            logits = model(batch_data)\n            loss = criterion(logits, batch_labels)\n\n            running_val_loss += loss.item()\n            num_val_batches += 1\n            \n            probs = torch.softmax(logits, dim=1)\n            predictions = torch.argmax(probs, dim=1)\n            \n            y_val_preds.extend(predictions.cpu().numpy())\n            y_val_labels.extend(batch_labels.cpu().numpy())\n\n        avg_val_loss = running_val_loss / num_val_batches\n        \n        val_f1 = f1_score(y_val_labels, y_val_preds, zero_division=0)\n        val_precision = precision_score(y_val_labels, y_val_preds, zero_division=0)\n        val_recall = recall_score(y_val_labels, y_val_preds, zero_division=0)\n\n    \n    improved = False\n    if avg_val_loss < best_val_loss:\n        best_val_loss = avg_val_loss\n        best_val_f1 = val_f1 \n        patience_counter = 0\n        improved = True\n\n        torch.save({'model_state_dict': model.state_dict(), 'vocab': vocab, 'config': {\n            'vocab_size': len(vocab), 'embedding_dim': EMBEDDING_DIM, 'hidden_dim': HIDDEN_DIM,\n            'num_layers': NUM_LAYERS, 'dropout_rate': DROPOUT_RATE, 'pad_idx': vocab['<PAD>']\n        }}, 'best_model.pt')\n    else:\n        patience_counter += 1\n\n    elapsed = time.time() - start_time\n    status = \"Saved\" if improved else f\"({patience_counter}/{PATIENCE})\"\n\n    print(f\"Epoch {epoch+1:2d}/{epochs} | Status: {status}\")\n    print(f\"  Train Loss: {avg_train_loss:.4f} | F1: {train_f1:.4f} (P: {train_precision:.4f} R: {train_recall:.4f})\")\n    print(f\"  Val Loss:   {avg_val_loss:.4f} | F1: {val_f1:.4f} (P: {val_precision:.4f} R: {val_recall:.4f})\")\n\n    if patience_counter >= PATIENCE:\n        print(f\"\\nEarly stopping, validation not improving\")\n        break\n\n\nprint(\"Training complete.\")\nprint(\"\\n\")\n\ncheckpoint = torch.load('best_model.pt')\nmodel.load_state_dict(checkpoint['model_state_dict'])\nmodel.eval()\n\nprint(f\"\\nBest validation loss: {best_val_loss:.4f}\")\nprint(f\"Best validation F1: {best_val_f1:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T06:32:10.548274Z","iopub.execute_input":"2025-12-06T06:32:10.548943Z","iopub.status.idle":"2025-12-06T06:34:06.449505Z","shell.execute_reply.started":"2025-12-06T06:32:10.548915Z","shell.execute_reply":"2025-12-06T06:34:06.448752Z"}},"outputs":[{"name":"stdout","text":"Epoch  1/100 | Status: Saved\n  Train Loss: 0.5322 | F1: 0.7855 (P: 0.8515 R: 0.7289)\n  Val Loss:   0.4253 | F1: 0.8420 (P: 0.9233 R: 0.7740)\nEpoch  2/100 | Status: Saved\n  Train Loss: 0.4504 | F1: 0.8380 (P: 0.8927 R: 0.7896)\n  Val Loss:   0.3980 | F1: 0.8626 (P: 0.9231 R: 0.8096)\nEpoch  3/100 | Status: Saved\n  Train Loss: 0.4181 | F1: 0.8569 (P: 0.9070 R: 0.8120)\n  Val Loss:   0.3725 | F1: 0.8741 (P: 0.9316 R: 0.8233)\nEpoch  4/100 | Status: (1/5)\n  Train Loss: 0.3779 | F1: 0.8735 (P: 0.9210 R: 0.8306)\n  Val Loss:   0.3732 | F1: 0.8544 (P: 0.9538 R: 0.7737)\nEpoch  5/100 | Status: Saved\n  Train Loss: 0.3560 | F1: 0.8860 (P: 0.9264 R: 0.8489)\n  Val Loss:   0.3459 | F1: 0.9033 (P: 0.9170 R: 0.8900)\nEpoch  6/100 | Status: Saved\n  Train Loss: 0.3427 | F1: 0.8902 (P: 0.9322 R: 0.8518)\n  Val Loss:   0.3329 | F1: 0.8959 (P: 0.9361 R: 0.8590)\nEpoch  7/100 | Status: (1/5)\n  Train Loss: 0.3292 | F1: 0.8962 (P: 0.9370 R: 0.8587)\n  Val Loss:   0.3349 | F1: 0.8948 (P: 0.9337 R: 0.8590)\nEpoch  8/100 | Status: (2/5)\n  Train Loss: 0.3258 | F1: 0.8985 (P: 0.9374 R: 0.8627)\n  Val Loss:   0.3514 | F1: 0.9052 (P: 0.9141 R: 0.8964)\nEpoch  9/100 | Status: Saved\n  Train Loss: 0.3207 | F1: 0.9033 (P: 0.9383 R: 0.8708)\n  Val Loss:   0.3285 | F1: 0.9013 (P: 0.9345 R: 0.8703)\nEpoch 10/100 | Status: (1/5)\n  Train Loss: 0.3151 | F1: 0.9042 (P: 0.9404 R: 0.8708)\n  Val Loss:   0.3333 | F1: 0.8906 (P: 0.9400 R: 0.8460)\nEpoch 11/100 | Status: (2/5)\n  Train Loss: 0.3242 | F1: 0.9005 (P: 0.9418 R: 0.8626)\n  Val Loss:   0.3393 | F1: 0.8876 (P: 0.9415 R: 0.8396)\nEpoch 12/100 | Status: (3/5)\n  Train Loss: 0.3193 | F1: 0.9023 (P: 0.9430 R: 0.8650)\n  Val Loss:   0.3497 | F1: 0.9017 (P: 0.9238 R: 0.8807)\nEpoch 13/100 | Status: (4/5)\n  Train Loss: 0.3303 | F1: 0.9015 (P: 0.9407 R: 0.8655)\n  Val Loss:   0.5448 | F1: 0.9024 (P: 0.8825 R: 0.9233)\nEpoch 14/100 | Status: (5/5)\n  Train Loss: 0.3307 | F1: 0.9005 (P: 0.9409 R: 0.8635)\n  Val Loss:   0.3372 | F1: 0.8974 (P: 0.9306 R: 0.8664)\n\nEarly stopping, validation not improving\nTraining complete.\n\n\n\nBest validation loss: 0.3285\nBest validation F1: 0.9013\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Testing","metadata":{}},{"cell_type":"code","source":"CONFIDENCE_THRESHOLD = 0.65\n\ndef predict(text, threshold=CONFIDENCE_THRESHOLD):\n    model.eval()\n    \n    indices = text_to_indices(text, vocab, MAX_SEQ_LEN)\n    padded = indices + [vocab['<PAD>']] * (MAX_SEQ_LEN - len(indices))\n    tensor = torch.tensor([padded], dtype=torch.long).to(DEVICE)\n    \n    with torch.no_grad():\n        logits = model(tensor, use_temperature=True)\n        probs = torch.softmax(logits, dim=1)\n        confidence = probs[0, 1].item()\n    \n    is_real = confidence > threshold\n    if is_real:\n        if confidence > 0.85:\n            priority = \"HIGH - Immediate attention\"\n        elif confidence > 0.70:\n            priority = \"MEDIUM - Review within hour\"\n        else:\n            priority = \"LOW - Review when possible\"\n        prediction = \"REAL DISASTER\"\n    else:\n        priority = \"IGNORE - Not real disaster\"\n        prediction = \"FAKE/ABSURD\"\n    \n    return {\n        'prediction': prediction,\n        'confidence': confidence,\n        'priority': priority\n    }\n\ntest_cases = [\n    \"Mandatory evacuation order for coastal zones due to Category 4 storm surge.\",\n    \"Official update: 8 confirmed fatalities and critical infrastructure damaged by the flooding.\",\n    \"Hospital running low on supplies after the storm; desperately need bandages and generators.\",\n    \"ALERT: The government is shutting down the internet due to a fake solar flare. Stock up now!\",\n    \"So excited for the new movie tonight! I heard the CGI is absolutely insane.\",\n    \"Giant radioactive snail terrorizes Tokyo; please stay indoors and avoid salty snacks.\",\n    \"Massive power outage downtown. Traffic lights are out, total gridlock.\",\n    \"Thinking of everyone affected by the California fires. Stay safe and strong! üôè\"\n]\n\nfor text in test_cases:\n    result = predict(text)\n    print(f\"\\n{text}\")\n    print(f\"{result['prediction']} ({result['confidence']:.1%})\")\n    print(f\"{result['priority']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T06:34:16.432202Z","iopub.execute_input":"2025-12-06T06:34:16.433087Z","iopub.status.idle":"2025-12-06T06:34:16.473906Z","shell.execute_reply.started":"2025-12-06T06:34:16.433062Z","shell.execute_reply":"2025-12-06T06:34:16.473297Z"}},"outputs":[{"name":"stdout","text":"\nMandatory evacuation order for coastal zones due to Category 4 storm surge.\nREAL DISASTER (95.9%)\nHIGH - Immediate attention\n\nOfficial update: 8 confirmed fatalities and critical infrastructure damaged by the flooding.\nREAL DISASTER (95.8%)\nHIGH - Immediate attention\n\nHospital running low on supplies after the storm; desperately need bandages and generators.\nREAL DISASTER (93.4%)\nHIGH - Immediate attention\n\nALERT: The government is shutting down the internet due to a fake solar flare. Stock up now!\nFAKE/ABSURD (20.5%)\nIGNORE - Not real disaster\n\nSo excited for the new movie tonight! I heard the CGI is absolutely insane.\nFAKE/ABSURD (4.2%)\nIGNORE - Not real disaster\n\nGiant radioactive snail terrorizes Tokyo; please stay indoors and avoid salty snacks.\nFAKE/ABSURD (5.8%)\nIGNORE - Not real disaster\n\nMassive power outage downtown. Traffic lights are out, total gridlock.\nFAKE/ABSURD (35.0%)\nIGNORE - Not real disaster\n\nThinking of everyone affected by the California fires. Stay safe and strong! üôè\nFAKE/ABSURD (15.4%)\nIGNORE - Not real disaster\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}